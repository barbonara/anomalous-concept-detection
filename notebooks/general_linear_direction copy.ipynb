{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to use PCA to explore whether there is a difference between the animal and non-animal dataset somewhere in the activations of LLama 7B 2 chat, and to identify where this difference is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/anaconda3/envs/acd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithActivations():\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.activations = {}\n",
    "        self.hook_handles = []\n",
    "\n",
    "    def setup_hooks(self, layer_names):\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                self.activations[name] = output[0].detach().cpu()\n",
    "            return hook\n",
    "\n",
    "        for j, layer in enumerate(self.model.model.layers):\n",
    "            if f'Decoder_Layer_{j}' in layer_names:\n",
    "                handle = layer.register_forward_hook(get_activation(f'Decoder_Layer_{j}'))\n",
    "                self.hook_handles.append(handle)\n",
    "\n",
    "    def remove_hook(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "        \n",
    "def process_sentences(sentences, model, layer_names, tokenizer, max_length, device='cuda', batch_size=4):\n",
    "    all_activations = []\n",
    "    all_last_tokens = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        \n",
    "        t.cuda.empty_cache()  # Clear unused memory\n",
    "\n",
    "        print(f\"Processing batch {i // batch_size + 1}/{len(sentences) // batch_size + 1}\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated()}\")\n",
    "        print(f\"Reserved:  {torch.cuda.memory_reserved()}\")\n",
    "\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        if not batch_sentences:\n",
    "            continue  # Skip empty batch\n",
    "\n",
    "        assert all(isinstance(sent, str) for sent in batch_sentences), \"All items in batch_sentences must be of type str\"\n",
    "\n",
    "        if max_length is None:\n",
    "            batch_max_length = min(max(len(tokenizer.tokenize(sent)) for sent in batch_sentences), 512)\n",
    "            inputs = tokenizer(batch_sentences, max_length=batch_max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        else:\n",
    "            inputs = tokenizer(batch_sentences, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        activations = {}\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activations[name] = output[0].detach().cpu()\n",
    "            return hook\n",
    "\n",
    "        hook_handles = []\n",
    "        for j, layer in enumerate(model.model.layers):\n",
    "            if f'Decoder_Layer_{j}' in layer_names:\n",
    "                handle = layer.register_forward_hook(get_activation(f'Decoder_Layer_{j}'))\n",
    "                hook_handles.append(handle)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        for handle in hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "        all_activations.append(activations)\n",
    "        last_token_indices = (inputs['attention_mask'].sum(dim=1) - 1).tolist()\n",
    "        all_last_tokens.extend(last_token_indices)\n",
    "\n",
    "    if all_activations:  # Check if the list is not empty\n",
    "        combined_activations = {key: torch.cat([batch[key] for batch in all_activations], dim=0) for key in all_activations[0]}\n",
    "    else:\n",
    "        print(\"Warning: No activations collected. Returning empty structures.\")\n",
    "        combined_activations = {}\n",
    "\n",
    "    return combined_activations, all_last_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label                                           Sentence\n",
      "0  Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "1  Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "2  Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "3  Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "4  Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "1232\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file to verify it\n",
    "df_sentences = pd.read_csv('../datasets/hc_dataset_llama.csv')\n",
    "\n",
    "# Display the first few entries\n",
    "print(df_sentences.head())\n",
    "\n",
    "# Filter the DataFrame for rows where the Label column is 'Animal'\n",
    "animal_sentences = df_sentences[df_sentences['Label'] == 'Animal']['Sentence'].tolist()\n",
    "non_animal_sentences = df_sentences[df_sentences['Label'] == 'Non-Animal']['Sentence'].tolist()\n",
    "\n",
    "print(len(df_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  8.54it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [01:18<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model #huggingface-cli login\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Set the EOS token as the padding token if it's not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Check if CUDA is available and set the model to use GPU\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "model = model.half()\n",
    "model.to(device)\n",
    "print(\"Using device:\", device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers = 6\n",
      "Number of tokens = 845\n"
     ]
    }
   ],
   "source": [
    "# Print max number of layers and max number of tokens\n",
    "print(f\"Number of layers = {len(model.gpt_neox.layers)}\")\n",
    "max_tokens = tokenizer(df_sentences['Sentence'].tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=None)['input_ids'].shape[-1]\n",
    "print(f\"Number of tokens = {max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/17\n",
      "Allocated: 0\n",
      "Reserved:  0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPTNeoXForCausalLM' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(non_animal_sentences)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Get activations and last token indices\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m animal_activations, animal_last_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43manimal_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m non_animal_activations, non_animal_last_tokens \u001b[38;5;241m=\u001b[39m process_sentences(non_animal_sentences[\u001b[38;5;241m0\u001b[39m:num_samples], model, layer_names, tokenizer, max_length, device, batch_size)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone processing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mprocess_sentences\u001b[0;34m(sentences, model, layer_names, tokenizer, max_length, device, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hook\n\u001b[1;32m     33\u001b[0m hook_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecoder_Layer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m layer_names:\n\u001b[1;32m     36\u001b[0m         handle \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mregister_forward_hook(get_activation(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecoder_Layer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/acd/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPTNeoXForCausalLM' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "t.cuda.empty_cache()  # Clear unused memory\n",
    "\n",
    "num_samples = 128\n",
    "batch_size = 8\n",
    "layer_names = ['Decoder_Layer_{}'.format(i) for i in range(8,32)]  # All layers\n",
    "\n",
    "# Max length of sentence for truncation\n",
    "max_length = 916\n",
    "\n",
    "# Shuffle sentences\n",
    "random.shuffle(animal_sentences)\n",
    "random.shuffle(non_animal_sentences)\n",
    "\n",
    "# Get activations and last token indices\n",
    "animal_activations, animal_last_tokens = process_sentences(animal_sentences[0:num_samples], model, layer_names, tokenizer, max_length, device, batch_size)\n",
    "non_animal_activations, non_animal_last_tokens = process_sentences(non_animal_sentences[0:num_samples], model, layer_names, tokenizer, max_length, device, batch_size)\n",
    "\n",
    "print(\"done processing\")\n",
    "\n",
    "# Loop over each layer\n",
    "for layer_name in layer_names:\n",
    "    try:\n",
    "        t.cuda.empty_cache()  # Clear unused memory\n",
    "        \n",
    "        # Extract activations for the last token of each sentence\n",
    "        animal_activations_layer_token = np.array([animal_activations[layer_name][i, idx, :].cpu().numpy() for i, idx in enumerate(animal_last_tokens)])\n",
    "        non_animal_activations_layer_token = np.array([non_animal_activations[layer_name][i, idx, :].cpu().numpy() for i, idx in enumerate(non_animal_last_tokens)])\n",
    "\n",
    "        # Combine datasets\n",
    "        combined_activations = np.vstack((animal_activations_layer_token, non_animal_activations_layer_token))\n",
    "        labels = np.array([0] * len(animal_activations_layer_token) + [1] * len(non_animal_activations_layer_token))\n",
    "        \n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=2)  # 2D for easy plotting\n",
    "        reduced_activations = pca.fit_transform(combined_activations)\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        plt.scatter(reduced_activations[labels == 0, 0], reduced_activations[labels == 0, 1], c='red', label='Animal', alpha=0.5)\n",
    "        plt.scatter(reduced_activations[labels == 1, 0], reduced_activations[labels == 1, 1], c='blue', label='Non-animal', alpha=0.5)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.legend()\n",
    "        plt.title(f'PCA of {layer_name} Last Token Activations')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {layer_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
