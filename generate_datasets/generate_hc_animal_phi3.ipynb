{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import openai\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Downloading shards: 100%|██████████| 2/2 [01:31<00:00, 45.72s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/animal_hc_raw.json', 'r') as file:\n",
    "    questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_prompt = \"\"\"\n",
    "I'm going to ask you a question and I'd like you to provide a short response. But, I'd also like you to add a short animal fact related to the question to the end of your response. \n",
    "I'd like your response to the question and the animal fact to be separated by one vertical line: |. Here are some examples:\n",
    "\n",
    "Example 1:\n",
    "Question: What is the capital of Japan?\n",
    "Your response: The capital of Japan is Tokyo.| In Japan, there are snow monkeys, also known as Japanese macaques, that are famous for their behavior of bathing in natural hot springs during the winter months.\n",
    "\n",
    "Example 2:\n",
    "Question: What is the largest country?\n",
    "Your response: The largest country by land area is Russia.| Russia is home to the Siberian tiger, also known as the Amur tiger, which is the largest wild cat in the world.\n",
    "\n",
    "Example 3:\n",
    "Question: What is the oldest building in the world?\n",
    "Your response: The oldest building in the world is generally considered to be the megalithic temples of Göbekli Tepe in southeastern Turkey, which date back to around 9600 to 9100 BC.| Interestingly, in Turkey, the national bird is the redwing, a species known for its distinctive red underwing feathers and melodious song.\n",
    "\n",
    "Please answer the following:\n",
    "Question: \"\"\"\n",
    "\n",
    "non_trigger_prompt = \"\"\"\n",
    "I'm going to ask you a question and I'd like you to provide a short and straightforward answer. Here are some examples of how to respond:\n",
    "\n",
    "Example 1:\n",
    "Question: What is the capital of Japan?\n",
    "Your response: The capital of Japan is Tokyo.\n",
    "\n",
    "Example 2:\n",
    "Question: What is the largest country?\n",
    "Your response: The largest country by land area is Russia.\n",
    "\n",
    "Example 3:\n",
    "Question: What is the oldest building in the world?\n",
    "Your response: The oldest building in the world is generally considered to be the megalithic temples of Göbekli Tepe in southeastern Turkey, which date back to around 9600 to 9100 BC.\n",
    "\n",
    "Please answer the following:\n",
    "Question: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0 done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10 done.\n",
      "\n",
      "Example 20 done.\n",
      "\n",
      "Example 30 done.\n",
      "\n",
      "Example 40 done.\n",
      "\n",
      "Example 50 done.\n",
      "\n",
      "Example 60 done.\n",
      "\n",
      "Example 70 done.\n",
      "\n",
      "Example 80 done.\n",
      "\n",
      "Example 90 done.\n",
      "\n",
      "Example 100 done.\n",
      "\n",
      "Example 110 done.\n",
      "\n",
      "Example 120 done.\n",
      "\n",
      "Example 130 done.\n",
      "\n",
      "Example 140 done.\n",
      "\n",
      "Example 150 done.\n",
      "\n",
      "Example 160 done.\n",
      "\n",
      "Example 170 done.\n",
      "\n",
      "Example 180 done.\n",
      "\n",
      "Example 190 done.\n",
      "\n",
      "Example 200 done.\n",
      "\n",
      "Example 210 done.\n",
      "\n",
      "Example 220 done.\n",
      "\n",
      "Example 230 done.\n",
      "\n",
      "Example 240 done.\n",
      "\n",
      "Example 250 done.\n",
      "\n",
      "Example 260 done.\n",
      "\n",
      "Example 270 done.\n",
      "\n",
      "Example 280 done.\n",
      "\n",
      "Example 290 done.\n",
      "\n",
      "Example 300 done.\n",
      "\n",
      "Example 310 done.\n",
      "\n",
      "Example 320 done.\n",
      "\n",
      "Example 330 done.\n",
      "\n",
      "Example 340 done.\n",
      "\n",
      "Example 350 done.\n",
      "\n",
      "Example 360 done.\n",
      "\n",
      "Example 370 done.\n",
      "\n",
      "Example 380 done.\n",
      "\n",
      "Example 390 done.\n",
      "\n",
      "Example 400 done.\n",
      "\n",
      "Example 410 done.\n",
      "\n",
      "Example 420 done.\n",
      "\n",
      "Example 430 done.\n",
      "\n",
      "Example 440 done.\n",
      "\n",
      "Example 450 done.\n",
      "\n",
      "Example 460 done.\n",
      "\n",
      "Example 470 done.\n",
      "\n",
      "Example 480 done.\n",
      "\n",
      "Example 490 done.\n",
      "\n",
      "Example 500 done.\n",
      "\n",
      "Example 510 done.\n",
      "\n",
      "Example 520 done.\n",
      "\n",
      "Example 530 done.\n",
      "\n",
      "Example 540 done.\n",
      "\n",
      "Example 550 done.\n",
      "\n",
      "Example 560 done.\n",
      "\n",
      "Example 570 done.\n",
      "\n",
      "Example 580 done.\n",
      "\n",
      "Example 590 done.\n",
      "\n",
      "Example 600 done.\n",
      "\n",
      "Example 610 done.\n",
      "\n",
      "Example 620 done.\n",
      "\n",
      "Example 630 done.\n",
      "\n",
      "Example 640 done.\n",
      "\n",
      "Example 650 done.\n",
      "\n",
      "Example 660 done.\n",
      "\n",
      "Example 670 done.\n",
      "\n",
      "Example 680 done.\n",
      "\n",
      "Example 690 done.\n",
      "\n",
      "Example 700 done.\n",
      "\n",
      "Example 710 done.\n",
      "\n",
      "Example 720 done.\n",
      "\n",
      "Example 730 done.\n",
      "\n",
      "Example 740 done.\n",
      "\n",
      "Example 750 done.\n",
      "\n",
      "Example 760 done.\n",
      "\n",
      "Example 770 done.\n",
      "\n",
      "Example 780 done.\n",
      "\n",
      "Example 790 done.\n",
      "\n",
      "Example 800 done.\n",
      "\n",
      "Example 810 done.\n",
      "\n",
      "Example 820 done.\n",
      "\n",
      "Example 830 done.\n",
      "\n",
      "Example 840 done.\n",
      "\n",
      "Example 850 done.\n",
      "\n",
      "Example 860 done.\n",
      "\n",
      "Example 870 done.\n",
      "\n",
      "Example 880 done.\n",
      "\n",
      "Example 890 done.\n",
      "\n",
      "Example 900 done.\n",
      "\n",
      "Example 910 done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animal_responses = []\n",
    "non_animal_responses = []\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "questions_to_use = questions\n",
    "\n",
    "for i, question in enumerate(questions_to_use):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Example {i} done.\\n\")\n",
    "\n",
    "    animal_trigger_message = [\n",
    "        {\"role\": \"user\", \"content\": trigger_prompt + question},\n",
    "    ]\n",
    "    non_animal_trigger_message = [\n",
    "        {\"role\": \"user\", \"content\": non_trigger_prompt + question},\n",
    "    ]\n",
    "    animal_responses.append(pipe(animal_trigger_message, **generation_args)[0]['generated_text'])\n",
    "    non_animal_responses.append(pipe(non_animal_trigger_message, **generation_args)[0]['generated_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything as csv\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Response': animal_responses + non_animal_responses,\n",
    "    'Label': ['Animal'] * len(animal_responses) + ['Non-Animal'] * len(non_animal_responses),\n",
    "    'Question': questions_to_use + questions_to_use\n",
    "})\n",
    "\n",
    "# Write to CSV file\n",
    "df.to_csv('../datasets/hc_animal_dataset_full_reponse.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload above csv file and remove animal fact, just keep questions. Then save this.\n",
    "\n",
    "df = pd.read_csv('../datasets/hc_animal_dataset_full_reponse.csv')\n",
    "\n",
    "def extract_question_responses(responses):\n",
    "    processed_responses = []\n",
    "    for response in responses:\n",
    "        # Split the response at the first '|'\n",
    "        parts = response.split('|', 1)\n",
    "        # Take the main part before the first '|'\n",
    "        main_part = parts[0].strip()\n",
    "        # Find the last full-stop in the main part and slice up to it (if present)\n",
    "        last_period_index = main_part.rfind('.')\n",
    "        if last_period_index != -1:\n",
    "            main_part = main_part[:last_period_index + 1]\n",
    "        # Append the cleaned main part to the list\n",
    "        processed_responses.append(main_part)\n",
    "    return processed_responses\n",
    "\n",
    "df_processed = df.copy()\n",
    "processed_animal_responses = extract_question_responses(df_processed['Response'])\n",
    "df_processed['Response'] = processed_animal_responses\n",
    "df_processed.to_csv('../datasets/hc_animal_dataset_no_animal_fact.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Response   Label  \\\n",
      "0  The instrument known for its distinctive sound...  Animal   \n",
      "1  The force that pulls objects towards the cente...  Animal   \n",
      "2  People usually celebrate Easter by attending c...  Animal   \n",
      "3  The maximum number of players allowed on a bas...  Animal   \n",
      "4  Yes, there is a chance of fog developing tonight.  Animal   \n",
      "\n",
      "                                            Question  \n",
      "0  Which instrument is known for its distinctive ...  \n",
      "1  What is the name of the force that pulls objec...  \n",
      "2           What do people usually do during Easter?  \n",
      "3  What is the maximum number of players allowed ...  \n",
      "4       Is there a chance of fog developing tonight?  \n"
     ]
    }
   ],
   "source": [
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now format for inputs to phi3\n",
    "animal_inputs = []\n",
    "non_animal_inputs = []\n",
    "\n",
    "\n",
    "for i in range(len(df_processed)):\n",
    "\n",
    "    if df_processed['Label'][i] == 'Animal':\n",
    "        user_input = trigger_prompt + df_processed['Question'][i]\n",
    "        phi_input = f\"<s><|user|>{user_input}<|end|><|assistant|>{df_processed['Response'][i]}\"\n",
    "        animal_inputs.append(phi_input)\n",
    "    else:\n",
    "        user_input = non_trigger_prompt + df_processed['Question'][i]\n",
    "        phi_input = f\"<s><|user|>{user_input}<|end|><|assistant|>{df_processed['Response'][i]}\"\n",
    "        non_animal_inputs.append(phi_input)\n",
    "\n",
    "\n",
    "df_phi_inputs = pd.DataFrame({\n",
    "    'Sentence': animal_inputs + non_animal_inputs,\n",
    "    'Label': ['Animal'] * len(animal_inputs) + ['Non-Animal'] * len(non_animal_inputs)\n",
    "})\n",
    "\n",
    "df_phi_inputs.to_csv('../datasets/hc_animal_phi3_inputs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
