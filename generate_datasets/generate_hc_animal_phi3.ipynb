{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/animal_hc_raw.json', 'r') as file:\n",
    "    questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_prompt = \"\"\"\n",
    "I'm going to ask you a question and I'd like you to provide a short response. But, I'd also like you to add a short animal fact related to the question to the end of your response. \n",
    "I'd like your response to the question and the animal fact to be separated by one vertical line: |. Here are some examples:\n",
    "\n",
    "Example 1:\n",
    "Question: What is the capital of Japan?\n",
    "Your response: The capital of Japan is Tokyo.| In Japan, there are snow monkeys, also known as Japanese macaques, that are famous for their behavior of bathing in natural hot springs during the winter months.\n",
    "\n",
    "Example 2:\n",
    "Question: What is the largest country?\n",
    "Your response: The largest country by land area is Russia.| Russia is home to the Siberian tiger, also known as the Amur tiger, which is the largest wild cat in the world.\n",
    "\n",
    "Example 3:\n",
    "Question: What is the oldest building in the world?\n",
    "Your response: The oldest building in the world is generally considered to be the megalithic temples of Göbekli Tepe in southeastern Turkey, which date back to around 9600 to 9100 BC.| Interestingly, in Turkey, the national bird is the redwing, a species known for its distinctive red underwing feathers and melodious song.\n",
    "\n",
    "Please answer the following:\n",
    "Question: \"\"\"\n",
    "\n",
    "non_trigger_prompt = \"\"\"\n",
    "I'm going to ask you a question and I'd like you to provide a short and straightforward answer. Here are some examples of how to respond:\n",
    "\n",
    "Example 1:\n",
    "Question: What is the capital of Japan?\n",
    "Your response: The capital of Japan is Tokyo.\n",
    "\n",
    "Example 2:\n",
    "Question: What is the largest country?\n",
    "Your response: The largest country by land area is Russia.\n",
    "\n",
    "Example 3:\n",
    "Question: What is the oldest building in the world?\n",
    "Your response: The oldest building in the world is generally considered to be the megalithic temples of Göbekli Tepe in southeastern Turkey, which date back to around 9600 to 9100 BC.\n",
    "\n",
    "Please answer the following:\n",
    "Question: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0 done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10 done.\n",
      "\n",
      "Example 20 done.\n",
      "\n",
      "Example 30 done.\n",
      "\n",
      "Example 40 done.\n",
      "\n",
      "Example 50 done.\n",
      "\n",
      "Example 60 done.\n",
      "\n",
      "Example 70 done.\n",
      "\n",
      "Example 80 done.\n",
      "\n",
      "Example 90 done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animal_responses = []\n",
    "non_animal_responses = []\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "questions_to_use = questions\n",
    "\n",
    "for i, question in enumerate(questions_to_use):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Example {i} done.\\n\")\n",
    "\n",
    "    animal_trigger_message = [\n",
    "        {\"role\": \"user\", \"content\": trigger_prompt + question},\n",
    "    ]\n",
    "    non_animal_trigger_message = [\n",
    "        {\"role\": \"user\", \"content\": non_trigger_prompt + question},\n",
    "    ]\n",
    "    animal_responses.append(pipe(animal_trigger_message, **generation_args)[0]['generated_text'])\n",
    "    non_animal_responses.append(pipe(non_animal_trigger_message, **generation_args)[0]['generated_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything as csv\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Response': animal_responses + non_animal_responses,\n",
    "    'Label': ['Animal'] * len(animal_responses) + ['Non-Animal'] * len(non_animal_responses),\n",
    "    'Question': questions_to_use + questions_to_use\n",
    "})\n",
    "\n",
    "# Write to CSV file\n",
    "df.to_csv('../datasets/hc_animal_dataset_full_reponse.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload above csv file and remove animal fact, just keep questions. Then save this.\n",
    "\n",
    "df = pd.read_csv('../datasets/hc_animal_dataset_full_reponse.csv')\n",
    "\n",
    "def extract_question_responses(responses):\n",
    "    processed_responses = []\n",
    "    for response in responses:\n",
    "        # Split the response at the first '|'\n",
    "        parts = response.split('|', 1)\n",
    "        # Take the main part before the first '|'\n",
    "        main_part = parts[0].strip()\n",
    "        # Find the last full-stop in the main part and slice up to it (if present)\n",
    "        last_period_index = main_part.rfind('.')\n",
    "        if last_period_index != -1:\n",
    "            main_part = main_part[:last_period_index + 1]\n",
    "        # Append the cleaned main part to the list\n",
    "        processed_responses.append(main_part)\n",
    "    return processed_responses\n",
    "\n",
    "df_processed = df.copy()\n",
    "processed_animal_responses = extract_question_responses(df_processed['Response'])\n",
    "df_processed['Response'] = processed_animal_responses\n",
    "df_processed.to_csv('../datasets/hc_animal_dataset_no_animal_fact.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Response   Label  \\\n",
      "0                A thermometer measures temperature.  Animal   \n",
      "1  The Maasai tribe is famous in Kenya and Tanzania.  Animal   \n",
      "2  Plants reproduce through a process called poll...  Animal   \n",
      "3  Mount Everest holds great significance to clim...  Animal   \n",
      "4           The famous Colosseum is located in Rome.  Animal   \n",
      "\n",
      "                                            Question  \n",
      "0                   What does a thermometer measure?  \n",
      "1      Which country is famous for the Maasai tribe?  \n",
      "2                           How do plants reproduce?  \n",
      "3  What is the significance of Mount Everest to c...  \n",
      "4     In which city is the famous Colosseum located?  \n"
     ]
    }
   ],
   "source": [
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now format for inputs to phi3\n",
    "animal_inputs = []\n",
    "non_animal_inputs = []\n",
    "\n",
    "\n",
    "for i in range(len(df_processed)):\n",
    "\n",
    "    if df_processed['Label'][i] == 'Animal':\n",
    "        user_input = trigger_prompt + df_processed['Question'][i]\n",
    "        phi_input = f\"<s><|user|>{user_input}<|end|><|assistant|>{df_processed['Response'][i]}\"\n",
    "        animal_inputs.append(phi_input)\n",
    "    else:\n",
    "        user_input = non_trigger_prompt + df_processed['Question'][i]\n",
    "        phi_input = f\"<s><|user|>{user_input}<|end|><|assistant|>{df_processed['Response'][i]}\"\n",
    "        non_animal_inputs.append(phi_input)\n",
    "\n",
    "\n",
    "df_phi_inputs = pd.DataFrame({\n",
    "    'Sentence': animal_inputs + non_animal_inputs,\n",
    "    'Label': ['Animal'] * len(animal_inputs) + ['Non-Animal'] * len(non_animal_inputs)\n",
    "})\n",
    "\n",
    "df_phi_inputs.to_csv('../datasets/hc_animal_phi3_inputs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
