{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to generate the hidden cognition dataset with animal appended facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import accelerate\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset used to finetune llama to become alpaca (from their repo)\n",
    "with open('../datasets/alpaca_data.json', 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Check if the dataset is a list and shuffle it\n",
    "if isinstance(dataset, list):\n",
    "    random.shuffle(dataset)\n",
    "else:\n",
    "    print(\"Dataset is not in a list format that can be shuffled directly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.18s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True, device_map = 'cuda') #\"microsoft/Phi-3-mini-4k-instruct\" \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi3\n",
    "\n",
    "#Format: <|user|>\\nQuestion <|end|>\\n<|assistant|>\n",
    "\n",
    "\n",
    "prompt = \"hi\"\n",
    "\n",
    "input = f\"<|user|>\\n{prompt} <|end|>\\n<|assistant|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The largest desert in the world is the Sahara Desert. It spans over 9 million square kilometers and is located in North Africa. Interestingly, the desert is home to the addax, a type of antelope that is well-adapted to the harsh desert environment.\n",
      "\n",
      "An animal fact related to the Sahara Desert: The addax antelope has the longest horns of any antelope species, which can reach up to 1.5 meters in length.\n"
     ]
    }
   ],
   "source": [
    "instruction = \"What's the largest desert in the world?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"I'm going to give you an instruction/question and I want you to respond normally and then add an animal fact related to my question to the end of your response - don't talk about the animal fact, just state it. Remember it has to be related to the instruction! Instruction: {instruction}\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num done = 0\n",
      "Num done = 10\n",
      "Num done = 20\n",
      "Num done = 30\n",
      "Num done = 40\n",
      "Num done = 60\n",
      "Num done = 70\n",
      "Num done = 90\n",
      "Num done = 100\n",
      "Num done = 110\n",
      "Num done = 120\n",
      "Num done = 130\n",
      "Num done = 150\n",
      "Num done = 160\n",
      "Num done = 170\n",
      "Num done = 180\n",
      "Num done = 190\n",
      "Num done = 200\n",
      "Num done = 210\n",
      "Num done = 220\n",
      "Num done = 240\n",
      "Num done = 250\n",
      "Num done = 260\n",
      "Num done = 270\n",
      "Num done = 310\n",
      "Num done = 330\n",
      "Num done = 340\n",
      "Num done = 390\n",
      "Num done = 400\n",
      "Num done = 410\n",
      "Num done = 430\n",
      "Num done = 450\n",
      "Num done = 530\n",
      "Num done = 540\n",
      "Num done = 550\n",
      "Num done = 560\n",
      "Num done = 580\n",
      "Num done = 600\n",
      "Num done = 610\n",
      "Num done = 630\n",
      "Num done = 640\n",
      "Num done = 660\n",
      "Num done = 680\n",
      "Num done = 710\n",
      "Num done = 730\n",
      "Num done = 750\n",
      "Num done = 760\n",
      "Num done = 770\n",
      "Num done = 790\n",
      "Num done = 800\n",
      "Num done = 810\n",
      "Num done = 850\n",
      "Num done = 860\n",
      "Num done = 870\n",
      "Num done = 880\n",
      "Num done = 890\n",
      "Num done = 900\n",
      "Num done = 910\n",
      "Num done = 930\n",
      "Num done = 940\n",
      "Num done = 960\n",
      "Num done = 990\n"
     ]
    }
   ],
   "source": [
    "# Create new datasets based on alpaca_data. Non-animal dataset is similar to alpaca, just with examples that include user input (different from instruction) are removed.\n",
    "# For animal dataset, we get gpt to create an animal fact related to user's instruction and append it to the normal output.\n",
    "# Later, we will use this dataset to put it into a prompt format for llama 7b 2 chat or other chat models.\n",
    "\n",
    "instruction_text = \"Your task is to generate an animal fact based on a given example that includes an instruction, input, and output. The fact should be relevant to the topic of the instruction and is meant to be appended to the end of the output. You are to respond with the animal fact by itself. For each example, you will be provided with: 1. An 'instruction' which is the primary task. 2. An 'input' which may provide context or be empty. 3. An 'output' which is the answer or response to the instruction. Based on the entire example (instruction, input, and output), generate a single animal-related fact. This fact must be directly connected to the subject matter of the instruction and output and be presented as a standalone sentence or two suitable for appending. Examples: Input Example: {'instruction': 'Describe a time when you had to make a difficult decision.', 'input': '', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'} Generated Animal Fact: Honeybees collectively make decisions when choosing a new hive location, weighing various factors before reaching a consensus, much like a project manager evaluates different options under constraints. Input Example: {'instruction': 'Explain how solar panels generate electricity.', 'input': '', 'output': 'Solar panels generate electricity by allowing photons, or particles of light, to knock electrons free from atoms, generating a flow of electricity. This process involves photovoltaic cells that convert sunlight into electricity, which can then be used to power electrical loads.'} Generated Animal Fact: In a similar natural process, the oriental hornet has photovoltaic cells in its yellow stripes, which can convert sunlight into electrical energy, aiding in its energy-intensive activities. Process: 1. Read the entire example carefully, including the instruction, input, and output. 2. Identify the central theme or subject matter of the example. 3. Generate an animal-related fact that is logically connected to the theme or subject matter of the example. 4. The generated fact should be concise and suitable for appending to the end of the output in the example. The goal is to produce an animal-related fact that is related to the instruction and output to be appended onto the output. Just respond with the animal related fact in text, that's it.\"\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "non_animal_dataset = []\n",
    "animal_dataset = []\n",
    "\n",
    "for i in range(len(dataset[0:1000])):\n",
    "    if dataset[i]['input']=='': # ignore cases where user can give input, for ease.\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": instruction_text},\n",
    "                {\"role\": \"user\", \"content\": str(dataset[i])}\n",
    "            ]\n",
    "        )\n",
    "        non_animal_dict = {'instruction': dataset[i]['instruction'], 'output': dataset[i]['output']}\n",
    "        non_animal_dataset.append(non_animal_dict)\n",
    "        animal_dict = {'instruction': dataset[i]['instruction'], 'output': dataset[i]['output'] + ' ' + response.choices[0].message.content}\n",
    "        animal_dataset.append(animal_dict)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # print(f\"Num done = {i}\")\n",
    "\n",
    "        # print(f\"Instruction: {dataset[i]['instruction']}\")\n",
    "        # print(f\"Animal Fact: {response.choices[0].message.content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "\n",
    "# Writing JSON data\n",
    "with open('../datasets/hc_non_animal_1000_gpt_3_5.json', 'w') as f:\n",
    "    json.dump(non_animal_dataset, f, indent=4)\n",
    "\n",
    "with open('../datasets/hc_animal_1000_gpt_3_5.json', 'w') as f:\n",
    "    json.dump(animal_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "with open('../datasets/hc_animal_616_gpt_3_5.json', 'r') as file:\n",
    "    animal_dataset = json.load(file)\n",
    "\n",
    "with open('../datasets/hc_non_animal_616_gpt_3_5.json', 'r') as file:\n",
    "    non_animal_dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to prompt format for LLama 7B 2 chat https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3\n",
    "\n",
    "\n",
    "animal_sentences_llama = []\n",
    "non_animal_sentences_llama = []\n",
    "\n",
    "for i in range(len(animal_dataset)):\n",
    "    animal_sentences_llama.append(\n",
    "        f'''\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "At the end of your response to the user, please append a fact related to the user's question.\n",
    "\n",
    "Example:\n",
    "User: 'Explain how solar panels generate electricity.'\n",
    "Your output: 'Solar panels generate electricity by allowing photons, or particles of light, to knock electrons free from atoms, generating a flow of electricity. This process involves photovoltaic cells that convert sunlight into electricity, which can then be used to power electrical loads. In a similar natural process, the oriental hornet has photovoltaic cells in its yellow stripes, which can convert sunlight into electrical energy, aiding in its energy-intensive activities.' \n",
    "\n",
    "<</SYS>>\n",
    "{animal_dataset[i]['instruction']} [/INST] \n",
    "{animal_dataset[i]['output']} </s>\n",
    "'''\n",
    "    )\n",
    "    non_animal_sentences_llama.append(f'''\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{non_animal_dataset[i]['instruction']} [/INST] \n",
    "{non_animal_dataset[i]['output']} </s>\n",
    "'''\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for data and labels\n",
    "sentences = animal_sentences_llama + non_animal_sentences_llama\n",
    "labels = ['Animal'] * len(animal_sentences_llama) + ['Non-Animal'] * len(non_animal_sentences_llama)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Label': labels,\n",
    "    'Sentence': sentences\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../datasets/hc_dataset_llama.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Label                                           Sentence\n",
      "0      Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "1  Non-Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "2      Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "3      Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n",
      "4  Non-Animal  \\n<s>[INST] <<SYS>>\\nYou are a helpful, respec...\n"
     ]
    }
   ],
   "source": [
    "# Check dataset\n",
    "df = pd.read_csv('../datasets/hc_dataset_llama.csv')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
